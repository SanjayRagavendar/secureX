{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6492730,"sourceType":"datasetVersion","datasetId":3752264}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"Credit card fraud with Neural Network 100% accurcy","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# # THEN FEEL FREE TO DELETE THIS CELL.\n# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# # NOTEBOOK.\n# import kagglehub\n# nelgiriyewithana_credit_card_fraud_detection_dataset_2023_path = kagglehub.dataset_download('nelgiriyewithana/credit-card-fraud-detection-dataset-2023')\n\n# print('Data source import complete.')\n","metadata":{"id":"0BM5RaAJo0nK","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:13.658458Z","iopub.execute_input":"2025-02-22T01:46:13.658744Z","iopub.status.idle":"2025-02-22T01:46:13.662362Z","shell.execute_reply.started":"2025-02-22T01:46:13.658720Z","shell.execute_reply":"2025-02-22T01:46:13.661392Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 1. Import python libraries","metadata":{"id":"6anvxKA-o0nQ"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mlt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nfile_path = ''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        file_path = os.path.join(dirname, filename)\ntrain_df = pd.read_csv(file_path)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:13.697451Z","iopub.execute_input":"2025-02-22T01:46:13.697673Z","iopub.status.idle":"2025-02-22T01:46:33.438429Z","shell.execute_reply.started":"2025-02-22T01:46:13.697656Z","shell.execute_reply":"2025-02-22T01:46:33.437489Z"},"id":"Crwu_Whoo0nT","outputId":"2d32898f-d356-416b-ce45-2dcdbea4e718"},"outputs":[{"name":"stdout","text":"/kaggle/input/creditcard_2023.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%time\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:33.439487Z","iopub.execute_input":"2025-02-22T01:46:33.439804Z","iopub.status.idle":"2025-02-22T01:46:33.444655Z","shell.execute_reply.started":"2025-02-22T01:46:33.439781Z","shell.execute_reply":"2025-02-22T01:46:33.443989Z"},"id":"AsJzTwdEo0nV","outputId":"4efa138b-c1bf-4040-8be5-0ed91589dbea"},"outputs":[{"name":"stdout","text":"CPU times: user 42 µs, sys: 5 µs, total: 47 µs\nWall time: 51.5 µs\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 2. Data Analysis","metadata":{"id":"HI11FKfHo0nW"}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:33.446312Z","iopub.execute_input":"2025-02-22T01:46:33.446512Z","iopub.status.idle":"2025-02-22T01:46:33.543067Z","shell.execute_reply.started":"2025-02-22T01:46:33.446495Z","shell.execute_reply":"2025-02-22T01:46:33.542102Z"},"id":"Ln4GrLl4o0nW","outputId":"7268c97e-8562-4d67-fa67-481a1e35ccf8"},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 568630 entries, 0 to 568629\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      568630 non-null  int64  \n 1   V1      568630 non-null  float64\n 2   V2      568630 non-null  float64\n 3   V3      568630 non-null  float64\n 4   V4      568630 non-null  float64\n 5   V5      568630 non-null  float64\n 6   V6      568630 non-null  float64\n 7   V7      568630 non-null  float64\n 8   V8      568630 non-null  float64\n 9   V9      568630 non-null  float64\n 10  V10     568630 non-null  float64\n 11  V11     568630 non-null  float64\n 12  V12     568630 non-null  float64\n 13  V13     568630 non-null  float64\n 14  V14     568630 non-null  float64\n 15  V15     568630 non-null  float64\n 16  V16     568630 non-null  float64\n 17  V17     568630 non-null  float64\n 18  V18     568630 non-null  float64\n 19  V19     568630 non-null  float64\n 20  V20     568630 non-null  float64\n 21  V21     568630 non-null  float64\n 22  V22     568630 non-null  float64\n 23  V23     568630 non-null  float64\n 24  V24     568630 non-null  float64\n 25  V25     568630 non-null  float64\n 26  V26     568630 non-null  float64\n 27  V27     568630 non-null  float64\n 28  V28     568630 non-null  float64\n 29  Amount  568630 non-null  float64\n 30  Class   568630 non-null  int64  \ndtypes: float64(29), int64(2)\nmemory usage: 134.5 MB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# - 2.1 **Observations**\n\n- Dataset does not have null values\n- Dataset have all the values in numeric/ decimal format","metadata":{"id":"Zn6EpkVAo0nX"}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:33.544240Z","iopub.execute_input":"2025-02-22T01:46:33.544469Z","iopub.status.idle":"2025-02-22T01:46:33.577582Z","shell.execute_reply.started":"2025-02-22T01:46:33.544448Z","shell.execute_reply":"2025-02-22T01:46:33.576909Z"},"id":"KTx3cam7o0nX","outputId":"05fc9bd6-fefd-4a33-8347-a31f5205ae05"},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id        V1        V2        V3        V4        V5        V6        V7  \\\n0   0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n1   1  0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n2   2 -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n3   3 -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n4   4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n\n         V8        V9       V10       V11       V12       V13       V14  \\\n0 -0.130006  0.727159  0.637735 -0.987020  0.293438 -0.941386  0.549020   \n1 -0.133118  0.347452  0.529808  0.140107  1.564246  0.574074  0.627719   \n2 -0.095576 -0.261297  0.690708 -0.272985  0.659201  0.805173  0.616874   \n3 -0.065130 -0.205698  0.575231 -0.752581  0.737483  0.592994  0.559535   \n4 -0.212660  1.049921  0.968046 -1.203171  1.029577  1.439310  0.241454   \n\n        V15       V16       V17       V18       V19       V20       V21  \\\n0  1.804879  0.215598  0.512307  0.333644  0.124270  0.091202 -0.110552   \n1  0.706121  0.789188  0.403810  0.201799 -0.340687 -0.233984 -0.194936   \n2  3.069025 -0.577514  0.886526  0.239442 -2.366079  0.361652 -0.005020   \n3 -0.697664 -0.030669  0.242629  2.178616 -1.345060 -0.378223 -0.146927   \n4  0.153008  0.224538  0.366466  0.291782  0.445317  0.247237 -0.106984   \n\n        V22       V23       V24       V25       V26       V27       V28  \\\n0  0.217606 -0.134794  0.165959  0.126280 -0.434824 -0.081230 -0.151045   \n1 -0.605761  0.079469 -0.577395  0.190090  0.296503 -0.248052 -0.064512   \n2  0.702906  0.945045 -1.154666 -0.605564 -0.312895 -0.300258 -0.244718   \n3 -0.038212 -0.214048 -1.893131  1.003963 -0.515950 -0.165316  0.048424   \n4  0.729727 -0.161666  0.312561 -0.414116  1.071126  0.023712  0.419117   \n\n     Amount  Class  \n0  17982.10      0  \n1   6531.37      0  \n2   2513.54      0  \n3   5384.44      0  \n4  14278.97      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-0.260648</td>\n      <td>-0.469648</td>\n      <td>2.496266</td>\n      <td>-0.083724</td>\n      <td>0.129681</td>\n      <td>0.732898</td>\n      <td>0.519014</td>\n      <td>-0.130006</td>\n      <td>0.727159</td>\n      <td>0.637735</td>\n      <td>-0.987020</td>\n      <td>0.293438</td>\n      <td>-0.941386</td>\n      <td>0.549020</td>\n      <td>1.804879</td>\n      <td>0.215598</td>\n      <td>0.512307</td>\n      <td>0.333644</td>\n      <td>0.124270</td>\n      <td>0.091202</td>\n      <td>-0.110552</td>\n      <td>0.217606</td>\n      <td>-0.134794</td>\n      <td>0.165959</td>\n      <td>0.126280</td>\n      <td>-0.434824</td>\n      <td>-0.081230</td>\n      <td>-0.151045</td>\n      <td>17982.10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.985100</td>\n      <td>-0.356045</td>\n      <td>0.558056</td>\n      <td>-0.429654</td>\n      <td>0.277140</td>\n      <td>0.428605</td>\n      <td>0.406466</td>\n      <td>-0.133118</td>\n      <td>0.347452</td>\n      <td>0.529808</td>\n      <td>0.140107</td>\n      <td>1.564246</td>\n      <td>0.574074</td>\n      <td>0.627719</td>\n      <td>0.706121</td>\n      <td>0.789188</td>\n      <td>0.403810</td>\n      <td>0.201799</td>\n      <td>-0.340687</td>\n      <td>-0.233984</td>\n      <td>-0.194936</td>\n      <td>-0.605761</td>\n      <td>0.079469</td>\n      <td>-0.577395</td>\n      <td>0.190090</td>\n      <td>0.296503</td>\n      <td>-0.248052</td>\n      <td>-0.064512</td>\n      <td>6531.37</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-0.260272</td>\n      <td>-0.949385</td>\n      <td>1.728538</td>\n      <td>-0.457986</td>\n      <td>0.074062</td>\n      <td>1.419481</td>\n      <td>0.743511</td>\n      <td>-0.095576</td>\n      <td>-0.261297</td>\n      <td>0.690708</td>\n      <td>-0.272985</td>\n      <td>0.659201</td>\n      <td>0.805173</td>\n      <td>0.616874</td>\n      <td>3.069025</td>\n      <td>-0.577514</td>\n      <td>0.886526</td>\n      <td>0.239442</td>\n      <td>-2.366079</td>\n      <td>0.361652</td>\n      <td>-0.005020</td>\n      <td>0.702906</td>\n      <td>0.945045</td>\n      <td>-1.154666</td>\n      <td>-0.605564</td>\n      <td>-0.312895</td>\n      <td>-0.300258</td>\n      <td>-0.244718</td>\n      <td>2513.54</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-0.152152</td>\n      <td>-0.508959</td>\n      <td>1.746840</td>\n      <td>-1.090178</td>\n      <td>0.249486</td>\n      <td>1.143312</td>\n      <td>0.518269</td>\n      <td>-0.065130</td>\n      <td>-0.205698</td>\n      <td>0.575231</td>\n      <td>-0.752581</td>\n      <td>0.737483</td>\n      <td>0.592994</td>\n      <td>0.559535</td>\n      <td>-0.697664</td>\n      <td>-0.030669</td>\n      <td>0.242629</td>\n      <td>2.178616</td>\n      <td>-1.345060</td>\n      <td>-0.378223</td>\n      <td>-0.146927</td>\n      <td>-0.038212</td>\n      <td>-0.214048</td>\n      <td>-1.893131</td>\n      <td>1.003963</td>\n      <td>-0.515950</td>\n      <td>-0.165316</td>\n      <td>0.048424</td>\n      <td>5384.44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-0.206820</td>\n      <td>-0.165280</td>\n      <td>1.527053</td>\n      <td>-0.448293</td>\n      <td>0.106125</td>\n      <td>0.530549</td>\n      <td>0.658849</td>\n      <td>-0.212660</td>\n      <td>1.049921</td>\n      <td>0.968046</td>\n      <td>-1.203171</td>\n      <td>1.029577</td>\n      <td>1.439310</td>\n      <td>0.241454</td>\n      <td>0.153008</td>\n      <td>0.224538</td>\n      <td>0.366466</td>\n      <td>0.291782</td>\n      <td>0.445317</td>\n      <td>0.247237</td>\n      <td>-0.106984</td>\n      <td>0.729727</td>\n      <td>-0.161666</td>\n      <td>0.312561</td>\n      <td>-0.414116</td>\n      <td>1.071126</td>\n      <td>0.023712</td>\n      <td>0.419117</td>\n      <td>14278.97</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:33.578249Z","iopub.execute_input":"2025-02-22T01:46:33.578502Z","iopub.status.idle":"2025-02-22T01:46:34.248611Z","shell.execute_reply.started":"2025-02-22T01:46:33.578482Z","shell.execute_reply":"2025-02-22T01:46:34.247810Z"},"id":"4-b5r3pgo0nY","outputId":"bb341cf9-ade3-47db-84ee-eaf87fe6ed5d"},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                  id            V1            V2            V3             V4  \\\ncount  568630.000000  5.686300e+05  5.686300e+05  5.686300e+05  568630.000000   \nmean   284314.500000 -5.118237e-17 -1.023647e-16  1.023647e-16       0.000000   \nstd    164149.486121  1.000001e+00  1.000001e+00  1.000001e+00       1.000001   \nmin         0.000000 -3.495584e+00 -4.996657e+01 -3.183760e+00      -4.951222   \n25%    142157.250000 -5.652859e-01 -4.866777e-01 -6.492987e-01      -0.656020   \n50%    284314.500000 -9.363846e-02 -1.358939e-01  3.528579e-04      -0.073762   \n75%    426471.750000  8.326582e-01  3.435552e-01  6.285380e-01       0.707005   \nmax    568629.000000  2.229046e+00  4.361865e+00  1.412583e+01       3.201536   \n\n                 V5            V6             V7             V8  \\\ncount  5.686300e+05  5.686300e+05  568630.000000  568630.000000   \nmean   2.559118e-17  2.559118e-17       0.000000       0.000000   \nstd    1.000001e+00  1.000001e+00       1.000001       1.000001   \nmin   -9.952786e+00 -2.111111e+01      -4.351839     -10.756342   \n25%   -2.934955e-01 -4.458712e-01      -0.283533      -0.192257   \n50%    8.108788e-02  7.871758e-02       0.233366      -0.114524   \n75%    4.397368e-01  4.977881e-01       0.525955       0.047299   \nmax    4.271689e+01  2.616840e+01     217.873038       5.958040   \n\n                  V9           V10            V11           V12           V13  \\\ncount  568630.000000  5.686300e+05  568630.000000  5.686300e+05  5.686300e+05   \nmean        0.000000  1.023647e-16       0.000000 -1.535471e-16 -1.279559e-17   \nstd         1.000001  1.000001e+00       1.000001  1.000001e+00  1.000001e+00   \nmin        -3.751919 -3.163276e+00      -5.954723 -2.020399e+00 -5.955227e+00   \n25%        -0.568745 -5.901008e-01      -0.701449 -8.311331e-01 -6.966667e-01   \n50%         0.092526  2.626145e-01      -0.041050  1.620521e-01  1.760812e-02   \n75%         0.559262  5.924603e-01       0.747773  7.446723e-01  6.856048e-01   \nmax        20.270062  3.172271e+01       2.513573  1.791356e+01  7.187486e+00   \n\n                V14           V15           V16            V17            V18  \\\ncount  5.686300e+05  5.686300e+05  5.686300e+05  568630.000000  568630.000000   \nmean  -5.118237e-17  3.198898e-17  5.118237e-17       0.000000       0.000000   \nstd    1.000001e+00  1.000001e+00  1.000001e+00       1.000001       1.000001   \nmin   -2.107417e+00 -3.861813e+00 -2.214513e+00      -2.484938      -2.421949   \n25%   -8.732057e-01 -6.212485e-01 -7.162655e-01      -0.619491      -0.556046   \n50%    2.305011e-01 -3.925566e-02  1.340262e-01       0.271641       0.087294   \n75%    7.518216e-01  6.654065e-01  6.556061e-01       0.518224       0.544389   \nmax    1.916954e+01  1.453220e+01  4.665291e+01       6.994124       6.783716   \n\n                 V19           V20           V21           V22           V23  \\\ncount  568630.000000  5.686300e+05  5.686300e+05  5.686300e+05  5.686300e+05   \nmean        0.000000 -2.559118e-17  1.279559e-17  1.599449e-18  6.397796e-18   \nstd         1.000001  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \nmin        -7.804988 -7.814784e+01 -1.938252e+01 -7.734798e+00 -3.029545e+01   \n25%        -0.565308 -3.502399e-01 -1.664408e-01 -4.904892e-01 -2.376289e-01   \n50%        -0.025979 -1.233776e-01 -3.743065e-02 -2.732881e-02 -5.968903e-02   \n75%         0.560116  2.482164e-01  1.479787e-01  4.638817e-01  1.557153e-01   \nmax         3.831672  2.987281e+01  8.087080e+00  1.263251e+01  3.170763e+01   \n\n                V24           V25            V26           V27           V28  \\\ncount  5.686300e+05  5.686300e+05  568630.000000  5.686300e+05  5.686300e+05   \nmean  -2.559118e-17 -3.838678e-17       0.000000 -2.559118e-17  3.838678e-17   \nstd    1.000001e+00  1.000001e+00       1.000001  1.000001e+00  1.000001e+00   \nmin   -4.067968e+00 -1.361263e+01      -8.226969 -1.049863e+01 -3.903524e+01   \n25%   -6.515801e-01 -5.541485e-01      -0.631895 -3.049607e-01 -2.318783e-01   \n50%    1.590123e-02 -8.193162e-03      -0.011892 -1.729111e-01 -1.392973e-02   \n75%    7.007374e-01  5.500147e-01       0.672888  3.340230e-01  4.095903e-01   \nmax    1.296564e+01  1.462151e+01       5.623285  1.132311e+02  7.725594e+01   \n\n              Amount     Class  \ncount  568630.000000  568630.0  \nmean    12041.957635       0.5  \nstd      6919.644449       0.5  \nmin        50.010000       0.0  \n25%      6054.892500       0.0  \n50%     12030.150000       0.5  \n75%     18036.330000       1.0  \nmax     24039.930000       1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>568630.000000</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>568630.000000</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>568630.000000</td>\n      <td>568630.000000</td>\n      <td>568630.000000</td>\n      <td>5.686300e+05</td>\n      <td>568630.000000</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>568630.000000</td>\n      <td>568630.000000</td>\n      <td>568630.000000</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>568630.000000</td>\n      <td>5.686300e+05</td>\n      <td>5.686300e+05</td>\n      <td>568630.000000</td>\n      <td>568630.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>284314.500000</td>\n      <td>-5.118237e-17</td>\n      <td>-1.023647e-16</td>\n      <td>1.023647e-16</td>\n      <td>0.000000</td>\n      <td>2.559118e-17</td>\n      <td>2.559118e-17</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.023647e-16</td>\n      <td>0.000000</td>\n      <td>-1.535471e-16</td>\n      <td>-1.279559e-17</td>\n      <td>-5.118237e-17</td>\n      <td>3.198898e-17</td>\n      <td>5.118237e-17</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2.559118e-17</td>\n      <td>1.279559e-17</td>\n      <td>1.599449e-18</td>\n      <td>6.397796e-18</td>\n      <td>-2.559118e-17</td>\n      <td>-3.838678e-17</td>\n      <td>0.000000</td>\n      <td>-2.559118e-17</td>\n      <td>3.838678e-17</td>\n      <td>12041.957635</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>164149.486121</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001</td>\n      <td>1.000001</td>\n      <td>1.000001</td>\n      <td>1.000001e+00</td>\n      <td>1.000001</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001</td>\n      <td>1.000001</td>\n      <td>1.000001</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>1.000001</td>\n      <td>1.000001e+00</td>\n      <td>1.000001e+00</td>\n      <td>6919.644449</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-3.495584e+00</td>\n      <td>-4.996657e+01</td>\n      <td>-3.183760e+00</td>\n      <td>-4.951222</td>\n      <td>-9.952786e+00</td>\n      <td>-2.111111e+01</td>\n      <td>-4.351839</td>\n      <td>-10.756342</td>\n      <td>-3.751919</td>\n      <td>-3.163276e+00</td>\n      <td>-5.954723</td>\n      <td>-2.020399e+00</td>\n      <td>-5.955227e+00</td>\n      <td>-2.107417e+00</td>\n      <td>-3.861813e+00</td>\n      <td>-2.214513e+00</td>\n      <td>-2.484938</td>\n      <td>-2.421949</td>\n      <td>-7.804988</td>\n      <td>-7.814784e+01</td>\n      <td>-1.938252e+01</td>\n      <td>-7.734798e+00</td>\n      <td>-3.029545e+01</td>\n      <td>-4.067968e+00</td>\n      <td>-1.361263e+01</td>\n      <td>-8.226969</td>\n      <td>-1.049863e+01</td>\n      <td>-3.903524e+01</td>\n      <td>50.010000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>142157.250000</td>\n      <td>-5.652859e-01</td>\n      <td>-4.866777e-01</td>\n      <td>-6.492987e-01</td>\n      <td>-0.656020</td>\n      <td>-2.934955e-01</td>\n      <td>-4.458712e-01</td>\n      <td>-0.283533</td>\n      <td>-0.192257</td>\n      <td>-0.568745</td>\n      <td>-5.901008e-01</td>\n      <td>-0.701449</td>\n      <td>-8.311331e-01</td>\n      <td>-6.966667e-01</td>\n      <td>-8.732057e-01</td>\n      <td>-6.212485e-01</td>\n      <td>-7.162655e-01</td>\n      <td>-0.619491</td>\n      <td>-0.556046</td>\n      <td>-0.565308</td>\n      <td>-3.502399e-01</td>\n      <td>-1.664408e-01</td>\n      <td>-4.904892e-01</td>\n      <td>-2.376289e-01</td>\n      <td>-6.515801e-01</td>\n      <td>-5.541485e-01</td>\n      <td>-0.631895</td>\n      <td>-3.049607e-01</td>\n      <td>-2.318783e-01</td>\n      <td>6054.892500</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>284314.500000</td>\n      <td>-9.363846e-02</td>\n      <td>-1.358939e-01</td>\n      <td>3.528579e-04</td>\n      <td>-0.073762</td>\n      <td>8.108788e-02</td>\n      <td>7.871758e-02</td>\n      <td>0.233366</td>\n      <td>-0.114524</td>\n      <td>0.092526</td>\n      <td>2.626145e-01</td>\n      <td>-0.041050</td>\n      <td>1.620521e-01</td>\n      <td>1.760812e-02</td>\n      <td>2.305011e-01</td>\n      <td>-3.925566e-02</td>\n      <td>1.340262e-01</td>\n      <td>0.271641</td>\n      <td>0.087294</td>\n      <td>-0.025979</td>\n      <td>-1.233776e-01</td>\n      <td>-3.743065e-02</td>\n      <td>-2.732881e-02</td>\n      <td>-5.968903e-02</td>\n      <td>1.590123e-02</td>\n      <td>-8.193162e-03</td>\n      <td>-0.011892</td>\n      <td>-1.729111e-01</td>\n      <td>-1.392973e-02</td>\n      <td>12030.150000</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>426471.750000</td>\n      <td>8.326582e-01</td>\n      <td>3.435552e-01</td>\n      <td>6.285380e-01</td>\n      <td>0.707005</td>\n      <td>4.397368e-01</td>\n      <td>4.977881e-01</td>\n      <td>0.525955</td>\n      <td>0.047299</td>\n      <td>0.559262</td>\n      <td>5.924603e-01</td>\n      <td>0.747773</td>\n      <td>7.446723e-01</td>\n      <td>6.856048e-01</td>\n      <td>7.518216e-01</td>\n      <td>6.654065e-01</td>\n      <td>6.556061e-01</td>\n      <td>0.518224</td>\n      <td>0.544389</td>\n      <td>0.560116</td>\n      <td>2.482164e-01</td>\n      <td>1.479787e-01</td>\n      <td>4.638817e-01</td>\n      <td>1.557153e-01</td>\n      <td>7.007374e-01</td>\n      <td>5.500147e-01</td>\n      <td>0.672888</td>\n      <td>3.340230e-01</td>\n      <td>4.095903e-01</td>\n      <td>18036.330000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>568629.000000</td>\n      <td>2.229046e+00</td>\n      <td>4.361865e+00</td>\n      <td>1.412583e+01</td>\n      <td>3.201536</td>\n      <td>4.271689e+01</td>\n      <td>2.616840e+01</td>\n      <td>217.873038</td>\n      <td>5.958040</td>\n      <td>20.270062</td>\n      <td>3.172271e+01</td>\n      <td>2.513573</td>\n      <td>1.791356e+01</td>\n      <td>7.187486e+00</td>\n      <td>1.916954e+01</td>\n      <td>1.453220e+01</td>\n      <td>4.665291e+01</td>\n      <td>6.994124</td>\n      <td>6.783716</td>\n      <td>3.831672</td>\n      <td>2.987281e+01</td>\n      <td>8.087080e+00</td>\n      <td>1.263251e+01</td>\n      <td>3.170763e+01</td>\n      <td>1.296564e+01</td>\n      <td>1.462151e+01</td>\n      <td>5.623285</td>\n      <td>1.132311e+02</td>\n      <td>7.725594e+01</td>\n      <td>24039.930000</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"fraud = train_df[train_df['Class'] == 1]\nnon_fraud = train_df[train_df['Class'] == 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:34.249401Z","iopub.execute_input":"2025-02-22T01:46:34.249714Z","iopub.status.idle":"2025-02-22T01:46:34.311827Z","shell.execute_reply.started":"2025-02-22T01:46:34.249685Z","shell.execute_reply":"2025-02-22T01:46:34.311114Z"},"id":"xcxBOeQno0nY"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(f\"Total records in dataset is : {train_df.shape[0]} and columns are {train_df.shape[1]}\")\nprint(f\"Total fraud records are : {len(fraud)}\")\nprint(f\"Total non=fraud records are : {len(non_fraud)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:34.312609Z","iopub.execute_input":"2025-02-22T01:46:34.312835Z","iopub.status.idle":"2025-02-22T01:46:34.318438Z","shell.execute_reply.started":"2025-02-22T01:46:34.312808Z","shell.execute_reply":"2025-02-22T01:46:34.317484Z"},"id":"K07XBaoUo0nY","outputId":"d34a37c7-c69b-4887-9988-34288c62d3e0"},"outputs":[{"name":"stdout","text":"Total records in dataset is : 568630 and columns are 31\nTotal fraud records are : 284315\nTotal non=fraud records are : 284315\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# - 2.2 **Observations**\n\n- Dataset does not have null values\n- Dataset have all the values in numeric/ decimal format\n- Because the records count is bit high, we will use sampling technique in ML, to train the model and this will also tells that how the data behaves with different models.\n- For Deep Learning (NN), we will directly use this data because NN works well on large datasets. Will convert data into train/ test with shuffle split so that our target values of 0 and 1 will get equally split in both. With this training and testing datasets will have right data.","metadata":{"id":"RsgK3sCro0nZ"}},{"cell_type":"markdown","source":"**NOTE : Lets split the data and take 1% of data seperate. Will check model accuracy after training with this. This data will not be envolve at any phase and will be used for prediction only.**","metadata":{"id":"tcHOKT4To0nZ"}},{"cell_type":"code","source":"# 0.1% of the data\npercentage = 0.1 / 100\nsubset_size = int(len(train_df) * percentage)\n\ndata_shuffled = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\ndata_for_prediction = data_shuffled.iloc[:subset_size]  # 0.1% of the data\nremaining_data = data_shuffled.iloc[subset_size:]  # The rest of the data\n\nprint(f\"Total rows: {len(train_df)}\")\nprint(f\"Subset size (0.1%): {len(data_for_prediction)}\")\nprint(f\"Remaining data size: {len(remaining_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:34.321611Z","iopub.execute_input":"2025-02-22T01:46:34.321891Z","iopub.status.idle":"2025-02-22T01:46:34.516725Z","shell.execute_reply.started":"2025-02-22T01:46:34.321869Z","shell.execute_reply":"2025-02-22T01:46:34.515985Z"},"id":"lq8yKGf-o0nZ","outputId":"e0bf769d-912c-41e8-be6f-a304e4ce2af6"},"outputs":[{"name":"stdout","text":"Total rows: 568630\nSubset size (0.1%): 568\nRemaining data size: 568062\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# - 2.3 **Observations**:\n\n- Finally we have 568062 records for further use and 568 records are to check the model performce. We will use these records after our model got train and tested on testing data.","metadata":{"id":"o3EsvwtGo0nZ"}},{"cell_type":"markdown","source":"# 3. Lets visualize the data and understand all columns.","metadata":{"id":"3yNYSCdXo0nZ"}},{"cell_type":"markdown","source":"# - 3.1 This is a dynamic graph. We can zoom in/out for data understanding.","metadata":{"id":"-c8Hrd-lo0na"}},{"cell_type":"code","source":"correlation_matrix = train_df.corr()  #'pearson' 'kendall'\n\n# Create dynamic heatmap using Plotly\nfig = px.imshow(\n    correlation_matrix,\n    text_auto=True,  # Show correlation values on the heatmap\n    color_continuous_scale='Viridis',  # Choose a color scale\n    title='Correlation Heatmap of Credit Card Fraud Dataset',\n    labels=dict(x=\"Features\", y=\"Features\", color=\"Correlation\")\n)\n\nfig.update_layout(\n    width=1250,\n    height=900,\n    margin=dict(l=5, r=5, t=50, b=30),  # padding around the plot\n    xaxis=dict(tickangle=45),  # Rotate x-axis labels\n    font=dict(size=14)  # Adjust font size\n)\n\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:34.517716Z","iopub.execute_input":"2025-02-22T01:46:34.517926Z","iopub.status.idle":"2025-02-22T01:46:37.200259Z","shell.execute_reply.started":"2025-02-22T01:46:34.517909Z","shell.execute_reply":"2025-02-22T01:46:37.199336Z"},"id":"BIonnhXbo0na","outputId":"d80fe7a0-0e5d-4775-91df-28b093c06fc5"},"outputs":[{"output_type":"display_data","data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"b5a9121f-9e15-4374-a6bd-26dfbcca63a4\" class=\"plotly-graph-div\" style=\"height:900px; width:1250px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b5a9121f-9e15-4374-a6bd-26dfbcca63a4\")) {                    Plotly.newPlot(                        \"b5a9121f-9e15-4374-a6bd-26dfbcca63a4\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"texttemplate\":\"%{z}\",\"x\":[\"id\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"],\"y\":[\"id\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"],\"z\":[[1.0,-0.39574063634609186,0.42426660015678397,-0.6636550126419013,0.6175535126803031,-0.268445303160255,-0.3879163737639754,-0.41428766184406407,0.1212824618720944,-0.5084272496381994,-0.578013906495389,0.5893206629502115,-0.6529397868088173,-0.076331306893578,-0.7093463998438976,-0.08000386104114378,-0.49425457118429583,-0.41722622620182,-0.34105610591863156,0.2162762501783075,0.1458031601156638,0.09794805061724872,0.036105766923371904,0.017594362413079333,-0.1166853729104812,0.005586088202669589,0.052126439444869895,0.18419464795371931,0.08682241260576518,0.0017100864799035004,0.8642831627799797],[-0.39574063634609186,1.0,-0.561183633954515,0.4844985401635455,-0.49896297594474187,0.5174624261560666,0.3547276680045884,0.5733809491871343,-0.2267571953509537,0.5489728302956037,0.5991075580411234,-0.5257971295395826,0.5807153777873986,-0.020567396328351664,0.4944271248881445,0.046001529231383155,0.6218843218813161,0.6057987995629993,0.577295832305899,-0.37780296913540695,-0.21916363301133282,-0.034668853998961,-0.07372902350632325,-0.06891661505324978,-0.014650683759079892,-0.008507702589561227,0.00928087451526517,-0.12277228528304256,0.07011054256854851,-0.0012802385605771758,-0.5057610150941871],[0.42426660015678397,-0.561183633954515,1.0,-0.6278104920364848,0.5796381202291374,-0.6316692854376971,-0.34104030260177565,-0.6940216318716017,0.19132105989542247,-0.5850951879971115,-0.6217979747279547,0.558863314513673,-0.5749345644329501,0.01280104248376915,-0.5232940284201145,-0.16132470872148305,-0.5343915691618217,-0.4958359650001614,-0.48216245511293815,0.20882075629936625,0.26370697596110493,-0.013569618322896368,0.03534567996690406,0.15190643498074394,-0.027515403471878426,0.13244340543166266,0.012219258985777469,0.0538349427899885,0.02107139018125639,-0.0000755599715205596,0.4918780282578429],[-0.6636550126419013,0.4844985401635455,-0.6278104920364848,1.0,-0.6877259494882653,0.5103506745545009,0.5089742765600225,0.6343364195731893,-0.26301834565830645,0.6486148696013958,0.7076758294754224,-0.6884356535465148,0.7054965815551926,-0.019272216573547565,0.6731790018512175,0.09851569153528388,0.614503523616856,0.5782232950904196,0.5255091135633305,-0.3143962041048633,-0.2538045108154867,-0.021710131774320938,-0.041969569923490935,-0.058883928967592265,0.07646049052956555,-0.07633191051182373,-0.05205570588789303,-0.19058190442642814,0.005346311771618318,-0.0020012079144899127,-0.682095464746486],[0.6175535126803031,-0.49896297594474187,0.5796381202291374,-0.6877259494882653,1.0,-0.4292428772856263,-0.47440322310037886,-0.5886482791490477,0.19901264056726828,-0.6766483622753064,-0.7128390725127359,0.7086423094474218,-0.7225967303170815,0.011518842515627868,-0.7148469911184303,-0.0986265870454093,-0.5939483014169327,-0.5327862103214046,-0.48226730151654923,0.2698417881195017,0.2572364467377895,-0.013093133699072597,0.09119724342710117,0.04326638430573659,-0.10250843572171739,0.02940248515640996,0.13667916220342638,0.1880359064117964,-0.011315747049474396,0.0018589045673618582,0.7359808565710273],[-0.268445303160255,0.5174624261560666,-0.6316692854376971,0.5103506745545009,-0.4292428772856263,1.0,0.2451865271667474,0.5868283406055932,-0.314975387956266,0.47961410205566885,0.5638742285705444,-0.44010003775567763,0.47300229956078726,-0.11531746775621386,0.3874539704529641,0.058686047016577425,0.5968984591889851,0.6696247633075968,0.6450950823825486,-0.4381178447989695,-0.24669410743890177,0.03414656449275871,-0.1191520154118044,-0.11391859759192685,-0.0832425180620607,-0.04784506633643524,0.04777113571618423,-0.0437589287691108,0.10842197466115198,-0.00001588941883052776,-0.33863896233138313],[-0.3879163737639754,0.3547276680045884,-0.34104030260177565,0.5089742765600225,-0.47440322310037886,0.2451865271667474,1.0,0.4187029281919174,-0.6044910167040766,0.43224090055475645,0.47100001966321764,-0.4976109474080235,0.4989928068037442,-0.11763678032145176,0.5101227871243331,-0.023851058571528735,0.41583445889451603,0.3781522888198369,0.3280191994444492,-0.235622710616555,-0.18835962602369546,-0.04015346339314524,0.036896294091701184,0.30859774569308235,-0.005236982679804255,-0.19534029155963678,-0.06760536585343863,-0.26078294959354836,-0.06564145984056691,0.0007340093466353905,-0.43508823958893617],[-0.41428766184406407,0.5733809491871343,-0.6940216318716017,0.6343364195731893,-0.5886482791490477,0.5868283406055932,0.4187029281919174,1.0,-0.18098596648968127,0.6017887092598622,0.6780040510354747,-0.5876600042753496,0.603318095143748,-0.030000475928052484,0.5356120570764168,0.13593907402214087,0.6672439794919695,0.6557547245191322,0.6256803922390054,-0.3722704884542475,-0.2994359231225608,0.019627330075980394,-0.10404342707656854,-0.11117683088820335,-0.004151657086955054,0.0008023286882098054,-0.006488061470276845,-0.03655730361393503,0.04073158661293576,0.0013257762708975484,-0.49123416156804167],[0.1212824618720944,-0.2267571953509537,0.19132105989542247,-0.26301834565830645,0.19901264056726828,-0.314975387956266,-0.6044910167040766,-0.18098596648968127,1.0,-0.2085567104208438,-0.19999499805519308,0.22305185834023264,-0.21199881899694165,0.27395800265287096,-0.21641026966863294,0.10169036535342921,-0.23063838987063393,-0.27724644253399855,-0.24998583730223237,0.2532717340963119,0.13135430045104124,0.05641560331642246,-0.09875159972309924,-0.4636485167240153,0.08327214499441321,0.3226394911997654,0.04044842457973952,0.29839817368877986,0.04601743546472396,-0.00020819584394533486,0.14429382694637913],[-0.5084272496381994,0.5489728302956037,-0.5850951879971115,0.6486148696013958,-0.6766483622753064,0.47961410205566885,0.43224090055475645,0.6017887092598622,-0.2085567104208438,1.0,0.7484867020106862,-0.6335560212487377,0.6672655813962111,-0.006167457161998786,0.633212342507672,0.11461275748373644,0.5739566182006085,0.5816038069117069,0.5227204786610877,-0.2944321078336574,-0.32897514531838673,0.1310009889880108,-0.20472327280093117,-0.04237105845413348,0.04400593554793419,-0.03488489122071076,-0.1309997704848007,-0.11184211963386996,0.06995860827816373,-0.0015887559777080344,-0.5855216713180037],[-0.578013906495389,0.5991075580411234,-0.6217979747279547,0.7076758294754224,-0.7128390725127359,0.5638742285705444,0.47100001966321764,0.6780040510354747,-0.19999499805519308,0.7484867020106862,1.0,-0.7130655812492382,0.7367831820400966,-0.01924632231706168,0.6989394067917468,0.11105135732356948,0.6866023339230601,0.6491491539838475,0.5967016678711734,-0.3750802694331134,-0.28705113069039656,0.037425698108432603,-0.1509565063762893,-0.05628475340944048,0.04593515384995922,-0.014045398258242862,-0.05368359245170589,-0.1349067088627371,0.035645954020764176,-0.001259392162245379,-0.6736650416630412],[0.5893206629502115,-0.5257971295395826,0.558863314513673,-0.6884356535465148,0.7086423094474218,-0.44010003775567763,-0.4976109474080235,-0.5876600042753496,0.22305185834023264,-0.6335560212487377,-0.7130655812492382,1.0,-0.7446416502342865,0.0145181359995506,-0.7623222709283751,-0.05808892470141749,-0.6550812990401549,-0.6019238103375801,-0.519721180696971,0.3461695726901959,0.20437798119881598,0.1116077050322859,0.02215343096562503,0.013595572422325498,-0.10433990365928231,0.05153542573834857,0.13363454230262947,0.29091186148961234,0.05973201344906711,0.00029212685372447574,0.7242781571398379],[-0.6529397868088173,0.5807153777873986,-0.5749345644329501,0.7054965815551926,-0.7225967303170815,0.47300229956078726,0.4989928068037442,0.603318095143748,-0.21199881899694165,0.6672655813962111,0.7367831820400966,-0.7446416502342865,1.0,0.018336369546867896,0.7838777077741362,0.04120815350775174,0.6984902814670928,0.6587385374736944,0.5793737934795381,-0.36816059030910053,-0.21927456488772207,-0.08039365688422824,-0.07209550059955189,-0.019261080067061898,0.0804069677600864,-0.010349933661424917,-0.11427224232866663,-0.216563239316756,-0.05313577795607689,-0.0012451681448115426,-0.7685793328121714],[-0.076331306893578,-0.020567396328351664,0.01280104248376915,-0.019272216573547565,0.011518842515627868,-0.11531746775621386,-0.11763678032145176,-0.030000475928052484,0.27395800265287096,-0.006167457161998786,-0.01924632231706168,0.0145181359995506,0.018336369546867896,1.0,0.02873800922395731,-0.016044189325415115,-0.08208900800066808,-0.12317344942842735,-0.12204932023033474,0.1676755323558825,-0.007267243745653744,0.02552939692896731,0.0020387295521381377,-0.12351981786734514,0.060096656979289224,0.00357978511467622,0.043750169281687915,0.058483250532414335,-0.10148753722101185,-0.002718124535265783,-0.0711051433546423],[-0.7093463998438976,0.4944271248881445,-0.5232940284201145,0.6731790018512175,-0.7148469911184303,0.3874539704529641,0.5101227871243331,0.5356120570764168,-0.21641026966863294,0.633212342507672,0.6989394067917468,-0.7623222709283751,0.7838777077741362,0.02873800922395731,1.0,0.01001084357105099,0.6305156380145103,0.5524280207131765,0.46939328487978893,-0.315192095442429,-0.1481037925353808,-0.18990202432483574,0.0520225573644203,-0.007601106060399176,0.13871751702173582,-0.08704029642874185,-0.14247177168658065,-0.2999506280161966,-0.1279686487234349,-0.0013632240339100058,-0.8056692033482222],[-0.08000386104114378,0.046001529231383155,-0.16132470872148305,0.09851569153528388,-0.0986265870454093,0.058686047016577425,-0.023851058571528735,0.13593907402214087,0.10169036535342921,0.11461275748373644,0.11105135732356948,-0.05808892470141749,0.04120815350775174,-0.016044189325415115,0.01001084357105099,1.0,0.0013574662955799714,0.03191240279952562,0.017135903043412365,0.18563822445563696,-0.13542338196326514,0.17171924136642103,-0.09934742297664327,-0.07483205218706489,0.023002977304329323,-0.027578825464532834,0.04783305835223175,0.11610630409017512,0.10029260664220709,0.001190145686198998,-0.037948155104996134],[-0.49425457118429583,0.6218843218813161,-0.5343915691618217,0.614503523616856,-0.5939483014169327,0.5968984591889851,0.41583445889451603,0.6672439794919695,-0.23063838987063393,0.5739566182006085,0.6866023339230601,-0.6550812990401549,0.6984902814670928,-0.08208900800066808,0.6305156380145103,0.0013574662955799714,1.0,0.8480951658049641,0.7679917806757725,-0.5854089849598509,-0.22343139181699778,-0.11759056483475493,-0.10184694645750264,-0.05710016326794003,-0.023510952780223037,0.06248428855094396,-0.05618446554371826,-0.19174166487061994,-0.022327748136707574,-0.00047902754350374703,-0.5735112908795752],[-0.41722622620182,0.6057987995629993,-0.4958359650001614,0.5782232950904196,-0.5327862103214046,0.6696247633075968,0.3781522888198369,0.6557547245191322,-0.27724644253399855,0.5816038069117069,0.6491491539838475,-0.6019238103375801,0.6587385374736944,-0.12317344942842735,0.5524280207131765,0.03191240279952562,0.8480951658049641,1.0,0.8513662967516887,-0.6142217577148429,-0.21570754669087858,-0.0793480029390729,-0.14463670835391573,-0.044634527596718376,-0.07219757703219588,0.07560930566543543,-0.045188799295263135,-0.1845499104307844,0.019569589896369564,-0.00035837864864404545,-0.47637650032080747],[-0.34105610591863156,0.577295832305899,-0.48216245511293815,0.5255091135633305,-0.48226730151654923,0.6450950823825486,0.3280191994444492,0.6256803922390054,-0.24998583730223237,0.5227204786610877,0.5967016678711734,-0.519721180696971,0.5793737934795381,-0.12204932023033474,0.46939328487978893,0.017135903043412365,0.7679917806757725,0.8513662967516887,1.0,-0.5659914961410238,-0.18566985395706914,-0.06086160098293352,-0.1359944775240973,-0.046262158695679886,-0.09974520968663647,0.07046725368044734,-0.021038556232283765,-0.14178954281092523,0.052547240046211176,-0.001516163720314371,-0.4100909344080857],[0.2162762501783075,-0.37780296913540695,0.20882075629936625,-0.3143962041048633,0.2698417881195017,-0.4381178447989695,-0.235622710616555,-0.3722704884542475,0.2532717340963119,-0.2944321078336574,-0.3750802694331134,0.3461695726901959,-0.36816059030910053,0.1676755323558825,-0.315192095442429,0.18563822445563696,-0.5854089849598509,-0.6142217577148429,-0.5659914961410238,1.0,0.07164076887416522,0.13607982697287904,0.11006570648336687,-0.0015291824800174388,0.11075074586832014,-0.17432823624942326,0.041420556221215474,0.12326603161983454,-0.02436821933283523,-0.00040037312534582946,0.24408145790406485],[0.1458031601156638,-0.21916363301133282,0.26370697596110493,-0.2538045108154867,0.2572364467377895,-0.24669410743890177,-0.18835962602369546,-0.2994359231225608,0.13135430045104124,-0.32897514531838673,-0.28705113069039656,0.20437798119881598,-0.21927456488772207,-0.007267243745653744,-0.1481037925353808,-0.13542338196326514,-0.22343139181699778,-0.21570754669087858,-0.18566985395706914,0.07164076887416522,1.0,-0.5299180843132985,0.4293617445941139,0.017203697191250335,-0.020316340494815607,0.030478239260711144,0.007677086709077101,-0.0551826715014143,-0.03572737591812128,-0.0014053479858549029,0.17985104660648765],[0.09794805061724872,-0.034668853998961,-0.013569618322896368,-0.021710131774320938,-0.013093133699072597,0.03414656449275871,-0.04015346339314524,0.019627330075980394,0.05641560331642246,0.1310009889880108,0.037425698108432603,0.1116077050322859,-0.08039365688422824,0.02552939692896731,-0.18990202432483574,0.17171924136642103,-0.11759056483475493,-0.0793480029390729,-0.06086160098293352,0.13607982697287904,-0.5299180843132985,1.0,-0.7346525004630902,0.09658722395900886,-0.05918971723348475,0.1461638879025279,0.07004976274161452,0.37325627407368606,0.32667695014452514,0.0010286257054379997,0.10964021915687452],[0.036105766923371904,-0.07372902350632325,0.03534567996690406,-0.041969569923490935,0.09119724342710117,-0.1191520154118044,0.036896294091701184,-0.10404342707656854,-0.09875159972309924,-0.20472327280093117,-0.1509565063762893,0.02215343096562503,-0.07209550059955189,0.0020387295521381377,0.0520225573644203,-0.09934742297664327,-0.10184694645750264,-0.14463670835391573,-0.1359944775240973,0.11006570648336687,0.4293617445941139,-0.7346525004630902,1.0,-0.0006357328469545662,0.07978970889777612,-0.2589562328030002,-0.015127211313114671,-0.3406395798078614,-0.2828929868641314,-0.0009421447402604272,0.014097879209649857],[0.017594362413079333,-0.06891661505324978,0.15190643498074394,-0.058883928967592265,0.04326638430573659,-0.11391859759192685,0.30859774569308235,-0.11117683088820335,-0.4636485167240153,-0.04237105845413348,-0.05628475340944048,0.013595572422325498,-0.019261080067061898,-0.12351981786734514,-0.007601106060399176,-0.07483205218706489,-0.05710016326794003,-0.044634527596718376,-0.046262158695679886,-0.0015291824800174388,0.017203697191250335,0.09658722395900886,-0.0006357328469545662,1.0,-0.05118060501814796,-0.040881589143863,0.0010565270344795445,-0.15169767279063637,0.028058555253005324,-0.001981201069020531,0.010254587927718118],[-0.1166853729104812,-0.014650683759079892,-0.027515403471878426,0.07646049052956555,-0.10250843572171739,-0.0832425180620607,-0.005236982679804255,-0.004151657086955054,0.08327214499441321,0.04400593554793419,0.04593515384995922,-0.10433990365928231,0.0804069677600864,0.060096656979289224,0.13871751702173582,0.023002977304329323,-0.023510952780223037,-0.07219757703219588,-0.09974520968663647,0.11075074586832014,-0.020316340494815607,-0.05918971723348475,0.07978970889777612,-0.05118060501814796,1.0,-0.0796039792185503,-0.11336223213049407,-0.1948991960259646,-0.04518932476338454,-0.0008457913911903768,-0.13010656532795353],[0.005586088202669589,-0.008507702589561227,0.13244340543166266,-0.07633191051182373,0.02940248515640996,-0.04784506633643524,-0.19534029155963678,0.0008023286882098054,0.3226394911997654,-0.03488489122071076,-0.014045398258242862,0.05153542573834857,-0.010349933661424917,0.00357978511467622,-0.08704029642874185,-0.027578825464532834,0.06248428855094396,0.07560930566543543,0.07046725368044734,-0.17432823624942326,0.030478239260711144,0.1461638879025279,-0.2589562328030002,-0.040881589143863,-0.0796039792185503,1.0,0.05754648491725454,0.21565272893161377,0.17605769656554718,-0.000720049256322993,0.06184695793743231],[0.052126439444869895,0.00928087451526517,0.012219258985777469,-0.05205570588789303,0.13667916220342638,0.04777113571618423,-0.06760536585343863,-0.006488061470276845,0.04044842457973952,-0.1309997704848007,-0.05368359245170589,0.13363454230262947,-0.11427224232866663,0.043750169281687915,-0.14247177168658065,0.04783305835223175,-0.05618446554371826,-0.045188799295263135,-0.021038556232283765,0.041420556221215474,0.007677086709077101,0.07004976274161452,-0.015127211313114671,0.0010565270344795445,-0.11336223213049407,0.05754648491725454,1.0,0.1939765758674741,0.036830432804852396,-0.00012047751268706506,0.07105198071170701],[0.18419464795371931,-0.12277228528304256,0.0538349427899885,-0.19058190442642814,0.1880359064117964,-0.0437589287691108,-0.26078294959354836,-0.03655730361393503,0.29839817368877986,-0.11184211963386996,-0.1349067088627371,0.29091186148961234,-0.216563239316756,0.058483250532414335,-0.2999506280161966,0.11610630409017512,-0.19174166487061994,-0.1845499104307844,-0.14178954281092523,0.12326603161983454,-0.0551826715014143,0.37325627407368606,-0.3406395798078614,-0.15169767279063637,-0.1948991960259646,0.21565272893161377,0.1939765758674741,1.0,0.18323342994142555,0.0012350522496339697,0.2140019431352946],[0.08682241260576518,0.07011054256854851,0.02107139018125639,0.005346311771618318,-0.011315747049474396,0.10842197466115198,-0.06564145984056691,0.04073158661293576,0.04601743546472396,0.06995860827816373,0.035645954020764176,0.05973201344906711,-0.05313577795607689,-0.10148753722101185,-0.1279686487234349,0.10029260664220709,-0.022327748136707574,0.019569589896369564,0.052547240046211176,-0.02436821933283523,-0.03572737591812128,0.32667695014452514,-0.2828929868641314,0.028058555253005324,-0.04518932476338454,0.17605769656554718,0.036830432804852396,0.18323342994142555,1.0,-0.0015028023500341778,0.10202373369321234],[0.0017100864799035004,-0.0012802385605771758,-0.0000755599715205596,-0.0020012079144899127,0.0018589045673618582,-0.00001588941883052776,0.0007340093466353905,0.0013257762708975484,-0.00020819584394533486,-0.0015887559777080344,-0.001259392162245379,0.00029212685372447574,-0.0012451681448115426,-0.002718124535265783,-0.0013632240339100058,0.001190145686198998,-0.00047902754350374703,-0.00035837864864404545,-0.001516163720314371,-0.00040037312534582946,-0.0014053479858549029,0.0010286257054379997,-0.0009421447402604272,-0.001981201069020531,-0.0008457913911903768,-0.000720049256322993,-0.00012047751268706506,0.0012350522496339697,-0.0015028023500341778,1.0,0.0022608304015472665],[0.8642831627799797,-0.5057610150941871,0.4918780282578429,-0.682095464746486,0.7359808565710273,-0.33863896233138313,-0.43508823958893617,-0.49123416156804167,0.14429382694637913,-0.5855216713180037,-0.6736650416630412,0.7242781571398379,-0.7685793328121714,-0.0711051433546423,-0.8056692033482222,-0.037948155104996134,-0.5735112908795752,-0.47637650032080747,-0.4100909344080857,0.24408145790406485,0.17985104660648765,0.10964021915687452,0.014097879209649857,0.010254587927718118,-0.13010656532795353,0.06184695793743231,0.07105198071170701,0.2140019431352946,0.10202373369321234,0.0022608304015472665,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Features: %{x}\\u003cbr\\u003eFeatures: %{y}\\u003cbr\\u003eCorrelation: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Features\"},\"tickangle\":45},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Features\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Correlation\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"title\":{\"text\":\"Correlation Heatmap of Credit Card Fraud Dataset\"},\"margin\":{\"l\":5,\"r\":5,\"t\":50,\"b\":30},\"font\":{\"size\":14},\"width\":1250,\"height\":900},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('b5a9121f-9e15-4374-a6bd-26dfbcca63a4');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# - 3.2 Observation\n\n- With the above graph we get to know that more the green color (near to 1) having more strong relation with Class column.","metadata":{"id":"NZVX0SXYo0na"}},{"cell_type":"markdown","source":"# 4. Split data","metadata":{"id":"vsMy_9H6o0na"}},{"cell_type":"markdown","source":"- Split data based on Class so that values of Class like 0's and 1's will get split equally in train and test datasets.","metadata":{"id":"DRMWRzuao0na"}},{"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\n# Perform the split\nfor train_index, test_index in sss.split(train_df, train_df['Class']):\n    train_set = train_df.iloc[train_index]\n    test_set = train_df.iloc[test_index]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:37.201117Z","iopub.execute_input":"2025-02-22T01:46:37.201421Z","iopub.status.idle":"2025-02-22T01:46:37.490870Z","shell.execute_reply.started":"2025-02-22T01:46:37.201390Z","shell.execute_reply":"2025-02-22T01:46:37.489950Z"},"id":"K9rKtDejo0na"},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"- Data preprations","metadata":{"id":"wYsxtJIvo0na"}},{"cell_type":"code","source":"X = train_set.drop(['id','Class'], axis=1)\ny = train_set['Class']\n\ntest_X = test_set.drop(['id','Class'], axis=1)\ntest_y = test_set['Class']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:37.491807Z","iopub.execute_input":"2025-02-22T01:46:37.492139Z","iopub.status.idle":"2025-02-22T01:46:37.540429Z","shell.execute_reply.started":"2025-02-22T01:46:37.492111Z","shell.execute_reply":"2025-02-22T01:46:37.539709Z"},"id":"iZdTMOR0o0na"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"X.shape, y.shape,test_X.shape,test_y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:37.541226Z","iopub.execute_input":"2025-02-22T01:46:37.541466Z","iopub.status.idle":"2025-02-22T01:46:37.546788Z","shell.execute_reply.started":"2025-02-22T01:46:37.541435Z","shell.execute_reply":"2025-02-22T01:46:37.545926Z"},"id":"DOqQiNaqo0na","outputId":"47d2dfa5-bf1b-4b22-af2a-c5b5139a298d"},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((454904, 29), (454904,), (113726, 29), (113726,))"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# - 4.1 Observations:\n\n- Finally we have 454904 records to train our model with 29 columns\n- 113726 records for model testing.","metadata":{"id":"slL6afB1o0nb"}},{"cell_type":"code","source":"scaler = StandardScaler()\ntrain_features = scaler.fit_transform(X)\ntest_features = scaler.transform(test_X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:37.547544Z","iopub.execute_input":"2025-02-22T01:46:37.547795Z","iopub.status.idle":"2025-02-22T01:46:37.787733Z","shell.execute_reply.started":"2025-02-22T01:46:37.547764Z","shell.execute_reply":"2025-02-22T01:46:37.786796Z"},"id":"qcbULYPPo0nb"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# 5. Model training, testing and checking accuracy","metadata":{"id":"egD0f4K5o0nb"}},{"cell_type":"markdown","source":"- **Neural Networks**","metadata":{"id":"0BzM8ogNo0nb"}},{"cell_type":"markdown","source":"# - 5.1 Hyperparams for NN\n\n- **dropout_layer1** : Dropout randomly disables a fraction of neurons during training, forcing the network to learn more robust features. This reduces overfitting(big advantage). This drops that much amount % of neurons.\n- **tf.keras.regularizers.l2** : Add L1 or L2 regularization to the Dense layers to penalize large weights, which can help reduce overfitting.\n- **hidden_layer1** : try changing number of neurons. It changes the model performace and also reduce the overfitting.\n- **Data Normalization Layer** : *batch_norm_layer = tf.keras.layers.BatchNormalization()*  Batch normalization normalizes the input to each layer, stabilizing training and improving generalization.\n- **EarlyStopping** : This stops our training to do not run on extra epochs. So saves our time and overfitting also. Use the below 2 techniques for better results:\n  - restore_best_weights=True  #to ensure the model reverts to the best weights observed during training\n  - monitor='val_loss'   ## Monitor validation loss\n","metadata":{"id":"laGXsIeIo0nb"}},{"cell_type":"code","source":"# Input Layer\ninput_layer = tf.keras.layers.Dense(64, activation='relu', input_shape=(train_features.shape[1],))\n\n# Hidden Layers with Regularization\nhidden_layer1 = tf.keras.layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.02))\nbatch_norm1 = tf.keras.layers.BatchNormalization()\nactivation1 = tf.keras.layers.LeakyReLU(alpha=0.1)\ndropout1 = tf.keras.layers.Dropout(0.4)\n\nhidden_layer2 = tf.keras.layers.Dense(16, kernel_regularizer=tf.keras.regularizers.l2(0.02))\nbatch_norm2 = tf.keras.layers.BatchNormalization()\nactivation2 = tf.keras.layers.LeakyReLU(alpha=0.1)\ndropout2 = tf.keras.layers.Dropout(0.4)\n\nhidden_layer3 = tf.keras.layers.Dense(8, kernel_regularizer=tf.keras.regularizers.l2(0.02))\nbatch_norm3 = tf.keras.layers.BatchNormalization()\nactivation3 = tf.keras.layers.LeakyReLU(alpha=0.1)\n\n# Output Layer\noutput_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n\n# Build the Model\nmodel = tf.keras.Sequential([\n    input_layer,\n\n    hidden_layer1, batch_norm1, activation1, dropout1,\n    hidden_layer2, batch_norm2, activation2, dropout2,\n    hidden_layer3, batch_norm3, activation3,\n\n    output_layer\n])\n\nearly_stopping = EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc')  # AUC for better fraud detection evaluation\n    ]\n)\nrecords = model.fit(train_features, y, epochs=10, batch_size=32, verbose=1,\n                    validation_data=(test_features, test_y), callbacks=[early_stopping])\nprint(\"Done\")\n# loss, accuracy = model.evaluate(test_features, test_y, verbose=1)\n# print(f\"Test Loss: {loss:.4f}\")\n# print(f\"Test Accuracy: {accuracy:.4f}\")\n\n# plt.plot(records.history['loss'])\n# plt.plot(records.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Validation'], loc='upper right')\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:46:37.788672Z","iopub.execute_input":"2025-02-22T01:46:37.788988Z","iopub.status.idle":"2025-02-22T01:52:13.580400Z","shell.execute_reply.started":"2025-02-22T01:46:37.788950Z","shell.execute_reply":"2025-02-22T01:52:13.579521Z"},"id":"UMTAXlHoo0nb"},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning:\n\nArgument `alpha` is deprecated. Use `negative_slope` instead.\n\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3ms/step - accuracy: 0.9632 - auc: 0.9902 - loss: 0.2310 - precision: 0.9748 - recall: 0.9506 - val_accuracy: 0.9925 - val_auc: 0.9992 - val_loss: 0.0507 - val_precision: 0.9943 - val_recall: 0.9907\nEpoch 2/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9899 - auc: 0.9989 - loss: 0.0548 - precision: 0.9904 - recall: 0.9894 - val_accuracy: 0.9965 - val_auc: 0.9996 - val_loss: 0.0355 - val_precision: 0.9956 - val_recall: 0.9975\nEpoch 3/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9925 - auc: 0.9992 - loss: 0.0462 - precision: 0.9928 - recall: 0.9923 - val_accuracy: 0.9974 - val_auc: 0.9995 - val_loss: 0.0350 - val_precision: 0.9964 - val_recall: 0.9984\nEpoch 4/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9936 - auc: 0.9992 - loss: 0.0441 - precision: 0.9934 - recall: 0.9937 - val_accuracy: 0.9981 - val_auc: 0.9996 - val_loss: 0.0282 - val_precision: 0.9964 - val_recall: 0.9999\nEpoch 5/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9943 - auc: 0.9993 - loss: 0.0397 - precision: 0.9938 - recall: 0.9947 - val_accuracy: 0.9978 - val_auc: 0.9996 - val_loss: 0.0264 - val_precision: 0.9974 - val_recall: 0.9982\nEpoch 6/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9949 - auc: 0.9993 - loss: 0.0369 - precision: 0.9941 - recall: 0.9957 - val_accuracy: 0.9979 - val_auc: 0.9997 - val_loss: 0.0248 - val_precision: 0.9975 - val_recall: 0.9984\nEpoch 7/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9954 - auc: 0.9994 - loss: 0.0360 - precision: 0.9946 - recall: 0.9962 - val_accuracy: 0.9987 - val_auc: 0.9998 - val_loss: 0.0211 - val_precision: 0.9977 - val_recall: 0.9996\nEpoch 8/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9960 - auc: 0.9994 - loss: 0.0326 - precision: 0.9954 - recall: 0.9967 - val_accuracy: 0.9981 - val_auc: 0.9997 - val_loss: 0.0271 - val_precision: 0.9971 - val_recall: 0.9992\nEpoch 9/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9960 - auc: 0.9994 - loss: 0.0324 - precision: 0.9953 - recall: 0.9967 - val_accuracy: 0.9984 - val_auc: 0.9995 - val_loss: 0.0247 - val_precision: 0.9972 - val_recall: 0.9996\nEpoch 10/10\n\u001b[1m14216/14216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.9961 - auc: 0.9995 - loss: 0.0310 - precision: 0.9954 - recall: 0.9969 - val_accuracy: 0.9969 - val_auc: 0.9994 - val_loss: 0.0305 - val_precision: 0.9984 - val_recall: 0.9955\nDone\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"results = model.evaluate(test_features, test_y, verbose=1)\nloss, accuracy, precision, recall, f1_score = results  # Adjust variable names accordingly\n\nprint(f\"Test Loss: {loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test F1-Score: {f1_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:59:27.405275Z","iopub.execute_input":"2025-02-22T01:59:27.405559Z","iopub.status.idle":"2025-02-22T01:59:32.526454Z","shell.execute_reply.started":"2025-02-22T01:59:27.405534Z","shell.execute_reply":"2025-02-22T01:59:32.525613Z"},"id":"rxBI768Yo0nb"},"outputs":[{"name":"stdout","text":"\u001b[1m3554/3554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9988 - auc: 0.9998 - loss: 0.0206 - precision: 0.9981 - recall: 0.9995\nTest Loss: 0.0211\nTest Accuracy: 0.9987\nTest Precision: 0.9977\nTest Recall: 0.9996\nTest F1-Score: 0.9998\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"print(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:56:35.993180Z","iopub.execute_input":"2025-02-22T01:56:35.993469Z","iopub.status.idle":"2025-02-22T01:56:35.997756Z","shell.execute_reply.started":"2025-02-22T01:56:35.993447Z","shell.execute_reply":"2025-02-22T01:56:35.997016Z"}},"outputs":[{"name":"stdout","text":"[0.021060306578874588, 0.9986546635627747, 0.997718095779419, 0.9995955228805542, 0.9997815489768982]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Output results:\n\n- As we can see that model accuracy is almost 100% and loss is very less. That means our model is perfectly build.\n- This graph tells us that how our epochs were keep on running and making our model best till, it did not found that the prediction results are similar as validation dataset.","metadata":{"id":"7xYsHa3To0nb"}},{"cell_type":"markdown","source":"# 6. Check results on unseen data by applying this model.","metadata":{"id":"AJ9pAz9_o0nc"}},{"cell_type":"markdown","source":"**Lets test this model with our pre-declared prediction data.**","metadata":{"id":"LHbDbcX1o0nc"}},{"cell_type":"code","source":"data_for_prediction.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:56:43.834323Z","iopub.execute_input":"2025-02-22T01:56:43.834649Z","iopub.status.idle":"2025-02-22T01:56:43.869203Z","shell.execute_reply.started":"2025-02-22T01:56:43.834619Z","shell.execute_reply":"2025-02-22T01:56:43.868509Z"},"id":"tp5oSz16o0nc","outputId":"1e824698-17bc-45aa-adaa-c2596aaf930a"},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"        id        V1        V2        V3        V4        V5        V6  \\\n0   437378  0.420468 -0.070194 -0.569266  0.191673 -0.009607  0.426903   \n1   504222 -0.238944  0.250929 -0.374408  0.152938 -0.105008 -0.039028   \n2     4794 -0.117796 -0.147961  2.130455 -0.325762  0.325616  0.271351   \n3   388411 -0.855315  0.137014 -0.628116  0.613733 -0.643573 -0.664283   \n4   424512  0.257686  0.035247 -0.203112  0.506745 -0.242235 -0.192608   \n5   123536 -0.413357 -0.450377  1.184996 -0.378135  1.009580  1.385973   \n6   333319 -0.905109  1.090565 -1.187502  1.927398 -1.209692 -1.471852   \n7   369666  0.517429 -0.016506 -0.225822  0.437507  0.045921  0.111977   \n8    62882 -0.330014 -0.161835  1.288293 -0.591877  0.289209 -0.283643   \n9   414847 -1.998296  2.563713 -1.922748  2.220722 -2.315012  0.163451   \n10   37898 -0.239565 -0.914608  2.236272 -0.153059  0.143797  0.637981   \n11  443820 -1.153168  0.398433 -1.029774  0.888771 -0.887774 -0.131860   \n12  382863 -2.397646  2.894551 -2.311456  1.245323 -2.908986 -2.658784   \n13   28344  1.048048 -0.559085  0.615327 -0.476877  0.077899  0.555793   \n14   35303 -0.247323 -0.211047  1.250432 -0.404014  0.372625  0.886330   \n15  174089  1.775570 -0.358267 -0.099641 -0.117799  0.441439 -0.471585   \n16  385153 -0.347976  0.351449 -0.665557  0.920434 -0.494854 -0.699403   \n17  464790 -1.433882  1.413067 -1.479843  1.761009 -1.723727 -1.397144   \n18   82367 -0.294221 -0.972846  1.972505 -1.439339 -0.306559  1.360050   \n19  229226  0.016539 -0.159624  0.607801 -1.126025  0.869994  0.277329   \n\n          V7        V8        V9       V10       V11       V12       V13  \\\n0  -0.356728  0.096143  0.077806 -0.673283  0.586506 -0.750942 -1.033356   \n1  -0.293004  0.133771 -0.591631 -0.651518  0.354661 -0.744777  0.331696   \n2   0.772625 -0.244342  1.240012  0.414358 -0.310268 -0.126386  2.950418   \n3  -0.880040  0.466586 -1.045508 -1.234377  1.133538 -1.177124  0.349663   \n4  -0.289297  0.044488 -0.396122 -0.370119  0.874225 -0.730394 -2.110598   \n5   0.525802 -0.026273  0.428828  0.614420  0.073380  1.288058 -0.602212   \n6  -1.480131  0.820961 -1.747814 -1.673500  1.844200 -1.919698  0.157106   \n7  -0.120557 -0.011100 -0.381664 -0.149901  0.482652 -0.580711 -1.199153   \n8   0.623377 -0.084072  0.080601  0.396721 -0.932837  0.612180 -0.103629   \n9  -2.788664 -1.133662 -2.497692 -2.322209  1.274140 -1.696605 -0.190928   \n10  0.430692 -0.160555 -0.179968  0.905365 -0.953598  0.544469  1.005150   \n11 -0.989098 -0.051896 -0.977661 -1.191869  1.265965 -1.217546 -1.535552   \n12 -2.240014  4.794864 -1.120960 -1.246921  1.090825 -1.011093  0.921382   \n13  0.272981 -0.122176 -0.119740  1.118909 -0.169065  0.710064 -0.395981   \n14  0.370228  0.071564  0.174766  0.484641 -0.602041  0.321745 -1.612295   \n15  0.738081 -0.259170  0.495893  0.782529 -1.288591  0.404075 -1.101746   \n16 -0.757664  0.199784 -1.116544 -1.106623  1.108233 -1.220094  0.982276   \n17 -1.784354  1.057339 -1.727034 -1.860526  1.553582 -1.858695  0.182378   \n18  0.800627 -0.043037  0.022677  0.339720 -0.099597  0.512430 -0.743583   \n19  1.238909 -0.341875  1.031074  0.303010 -1.116769  0.231496 -0.578925   \n\n         V14       V15       V16       V17       V18       V19       V20  \\\n0  -0.886895  0.543423 -1.075065 -0.665189 -1.149084 -0.168782  0.235116   \n1  -0.731682  0.603739 -1.053119 -0.787556 -1.092381  0.895043  0.332143   \n2   1.211172 -0.858423  0.055065  0.779386  0.093685  0.513854  0.110696   \n3  -1.254409 -1.237353 -1.409580 -1.626020 -1.458452  1.084589  0.552311   \n4  -0.634357  0.440263 -0.595804 -0.735223 -0.463177 -0.325932  0.124330   \n5   0.744269 -0.010914 -0.188847  0.734558 -0.497864 -0.947582 -0.954756   \n6  -1.716313  0.394367 -2.020690 -2.172109 -2.032614  1.907355  0.859102   \n7  -0.604071  0.081929  0.011340 -0.446074  0.139542 -0.260829  0.131684   \n8   1.097272  0.943783  0.604135  0.405033 -0.035865  0.048024 -0.160212   \n9  -0.993515 -1.318173 -1.629526 -1.788834 -1.798008  0.873544  2.070130   \n10  0.474800  1.673229 -0.499807  0.724782  1.503645  0.070229  0.399908   \n11 -1.230553  0.195721 -1.363702 -1.391726 -1.289715  1.461207 -0.227530   \n12 -0.870346 -0.111151 -1.193785 -1.401722 -1.438677  0.523435  1.433718   \n13  1.034372  0.469603 -0.124207  0.358027  1.669389 -1.292892 -0.783717   \n14  1.408992  1.475467  0.774833  0.330033  0.927388  0.488063  0.002038   \n15  1.419499  0.292122  0.265918  0.326166  0.135647 -0.553871 -0.496381   \n16 -1.392358 -0.103015 -1.309980 -1.539134 -1.380626  0.311462  0.816260   \n17 -1.316269 -1.488809 -1.945002 -2.150259 -2.206676  1.250607 -0.148611   \n18  0.413766 -1.371178  0.911294  0.743846 -0.468919 -0.312641  0.735708   \n19 -0.110102 -0.373550  0.392097  0.684862  0.537014 -0.086618 -0.155895   \n\n         V21       V22       V23       V24       V25       V26       V27  \\\n0   0.134969  0.070433  0.047770 -0.851622  0.102876 -0.375436  0.820807   \n1   0.192405  0.289441 -0.255187 -0.817462  0.308284  1.582688  0.574425   \n2  -0.271739 -0.404654 -0.121235  0.857659  0.541920  0.756534 -0.238177   \n3   0.405505  0.167560  0.446262 -0.205976  0.492582  0.658619  1.609128   \n4   0.162191  0.165912 -0.181999  0.331451  1.043095  0.029799  0.643273   \n5  -0.139289  0.633786  0.356072 -2.138775 -0.093777 -0.721594 -0.664833   \n6   0.757018  0.582996  0.269935  0.962761 -0.860951  0.885863  1.626802   \n7   0.071712 -0.060887 -0.281729 -1.003867  1.059399  0.416818  0.365278   \n8  -0.286617 -1.276665  0.294000  0.687970  0.151482 -0.168056 -0.218770   \n9  -2.119779  2.568182  1.578465  0.802100 -1.068481  0.383964 -2.780941   \n10 -0.099731  0.203341  0.361850  0.172179  0.371764  0.139097 -0.336160   \n11 -0.144973  0.412875  0.737041 -0.849788  0.205872 -0.166221  1.127098   \n12  0.562760 -1.587140 -1.146213  0.341678  2.997600 -0.511209  1.774353   \n13 -0.200045 -0.241792 -0.072130 -0.604005  0.709923 -0.573698 -0.179990   \n14 -0.146362 -0.598806 -0.161802 -1.998599  0.262482 -0.927780  0.008611   \n15 -0.055479  0.312707 -0.038118  0.073018  0.633971 -1.197648 -0.285051   \n16  0.357058  0.034090 -0.159199 -0.342399  1.486273  0.608609  1.727343   \n17  0.462182 -0.140126  0.479393  1.226030 -0.665870 -0.842231 -1.004705   \n18  0.056936  0.418919  0.799179 -0.553536  0.535914 -0.708620 -0.300779   \n19 -0.244091 -0.393633 -0.330166  0.721116 -0.319584 -0.589745 -0.973118   \n\n         V28    Amount  Class  \n0   0.665983   8633.18      1  \n1   0.478489  12299.55      1  \n2  -0.403038   5215.87      0  \n3  -0.025592  19282.98      1  \n4   0.736723  19114.27      1  \n5  -1.516420  12622.46      0  \n6   1.396159  21616.06      1  \n7   0.537678   2518.11      1  \n8  -0.032723  21222.90      0  \n9  -1.372040  11009.50      1  \n10 -0.325017   3200.31      0  \n11 -0.046447  10421.22      1  \n12  0.902366  17492.76      1  \n13 -0.076768   5876.72      0  \n14 -0.090029  20595.58      0  \n15 -0.251860   7484.70      0  \n16  1.143565   8184.55      1  \n17 -0.218917  15624.23      1  \n18  0.091472  15521.20      0  \n19 -1.317766  16194.41      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>437378</td>\n      <td>0.420468</td>\n      <td>-0.070194</td>\n      <td>-0.569266</td>\n      <td>0.191673</td>\n      <td>-0.009607</td>\n      <td>0.426903</td>\n      <td>-0.356728</td>\n      <td>0.096143</td>\n      <td>0.077806</td>\n      <td>-0.673283</td>\n      <td>0.586506</td>\n      <td>-0.750942</td>\n      <td>-1.033356</td>\n      <td>-0.886895</td>\n      <td>0.543423</td>\n      <td>-1.075065</td>\n      <td>-0.665189</td>\n      <td>-1.149084</td>\n      <td>-0.168782</td>\n      <td>0.235116</td>\n      <td>0.134969</td>\n      <td>0.070433</td>\n      <td>0.047770</td>\n      <td>-0.851622</td>\n      <td>0.102876</td>\n      <td>-0.375436</td>\n      <td>0.820807</td>\n      <td>0.665983</td>\n      <td>8633.18</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>504222</td>\n      <td>-0.238944</td>\n      <td>0.250929</td>\n      <td>-0.374408</td>\n      <td>0.152938</td>\n      <td>-0.105008</td>\n      <td>-0.039028</td>\n      <td>-0.293004</td>\n      <td>0.133771</td>\n      <td>-0.591631</td>\n      <td>-0.651518</td>\n      <td>0.354661</td>\n      <td>-0.744777</td>\n      <td>0.331696</td>\n      <td>-0.731682</td>\n      <td>0.603739</td>\n      <td>-1.053119</td>\n      <td>-0.787556</td>\n      <td>-1.092381</td>\n      <td>0.895043</td>\n      <td>0.332143</td>\n      <td>0.192405</td>\n      <td>0.289441</td>\n      <td>-0.255187</td>\n      <td>-0.817462</td>\n      <td>0.308284</td>\n      <td>1.582688</td>\n      <td>0.574425</td>\n      <td>0.478489</td>\n      <td>12299.55</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4794</td>\n      <td>-0.117796</td>\n      <td>-0.147961</td>\n      <td>2.130455</td>\n      <td>-0.325762</td>\n      <td>0.325616</td>\n      <td>0.271351</td>\n      <td>0.772625</td>\n      <td>-0.244342</td>\n      <td>1.240012</td>\n      <td>0.414358</td>\n      <td>-0.310268</td>\n      <td>-0.126386</td>\n      <td>2.950418</td>\n      <td>1.211172</td>\n      <td>-0.858423</td>\n      <td>0.055065</td>\n      <td>0.779386</td>\n      <td>0.093685</td>\n      <td>0.513854</td>\n      <td>0.110696</td>\n      <td>-0.271739</td>\n      <td>-0.404654</td>\n      <td>-0.121235</td>\n      <td>0.857659</td>\n      <td>0.541920</td>\n      <td>0.756534</td>\n      <td>-0.238177</td>\n      <td>-0.403038</td>\n      <td>5215.87</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>388411</td>\n      <td>-0.855315</td>\n      <td>0.137014</td>\n      <td>-0.628116</td>\n      <td>0.613733</td>\n      <td>-0.643573</td>\n      <td>-0.664283</td>\n      <td>-0.880040</td>\n      <td>0.466586</td>\n      <td>-1.045508</td>\n      <td>-1.234377</td>\n      <td>1.133538</td>\n      <td>-1.177124</td>\n      <td>0.349663</td>\n      <td>-1.254409</td>\n      <td>-1.237353</td>\n      <td>-1.409580</td>\n      <td>-1.626020</td>\n      <td>-1.458452</td>\n      <td>1.084589</td>\n      <td>0.552311</td>\n      <td>0.405505</td>\n      <td>0.167560</td>\n      <td>0.446262</td>\n      <td>-0.205976</td>\n      <td>0.492582</td>\n      <td>0.658619</td>\n      <td>1.609128</td>\n      <td>-0.025592</td>\n      <td>19282.98</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>424512</td>\n      <td>0.257686</td>\n      <td>0.035247</td>\n      <td>-0.203112</td>\n      <td>0.506745</td>\n      <td>-0.242235</td>\n      <td>-0.192608</td>\n      <td>-0.289297</td>\n      <td>0.044488</td>\n      <td>-0.396122</td>\n      <td>-0.370119</td>\n      <td>0.874225</td>\n      <td>-0.730394</td>\n      <td>-2.110598</td>\n      <td>-0.634357</td>\n      <td>0.440263</td>\n      <td>-0.595804</td>\n      <td>-0.735223</td>\n      <td>-0.463177</td>\n      <td>-0.325932</td>\n      <td>0.124330</td>\n      <td>0.162191</td>\n      <td>0.165912</td>\n      <td>-0.181999</td>\n      <td>0.331451</td>\n      <td>1.043095</td>\n      <td>0.029799</td>\n      <td>0.643273</td>\n      <td>0.736723</td>\n      <td>19114.27</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>123536</td>\n      <td>-0.413357</td>\n      <td>-0.450377</td>\n      <td>1.184996</td>\n      <td>-0.378135</td>\n      <td>1.009580</td>\n      <td>1.385973</td>\n      <td>0.525802</td>\n      <td>-0.026273</td>\n      <td>0.428828</td>\n      <td>0.614420</td>\n      <td>0.073380</td>\n      <td>1.288058</td>\n      <td>-0.602212</td>\n      <td>0.744269</td>\n      <td>-0.010914</td>\n      <td>-0.188847</td>\n      <td>0.734558</td>\n      <td>-0.497864</td>\n      <td>-0.947582</td>\n      <td>-0.954756</td>\n      <td>-0.139289</td>\n      <td>0.633786</td>\n      <td>0.356072</td>\n      <td>-2.138775</td>\n      <td>-0.093777</td>\n      <td>-0.721594</td>\n      <td>-0.664833</td>\n      <td>-1.516420</td>\n      <td>12622.46</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>333319</td>\n      <td>-0.905109</td>\n      <td>1.090565</td>\n      <td>-1.187502</td>\n      <td>1.927398</td>\n      <td>-1.209692</td>\n      <td>-1.471852</td>\n      <td>-1.480131</td>\n      <td>0.820961</td>\n      <td>-1.747814</td>\n      <td>-1.673500</td>\n      <td>1.844200</td>\n      <td>-1.919698</td>\n      <td>0.157106</td>\n      <td>-1.716313</td>\n      <td>0.394367</td>\n      <td>-2.020690</td>\n      <td>-2.172109</td>\n      <td>-2.032614</td>\n      <td>1.907355</td>\n      <td>0.859102</td>\n      <td>0.757018</td>\n      <td>0.582996</td>\n      <td>0.269935</td>\n      <td>0.962761</td>\n      <td>-0.860951</td>\n      <td>0.885863</td>\n      <td>1.626802</td>\n      <td>1.396159</td>\n      <td>21616.06</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>369666</td>\n      <td>0.517429</td>\n      <td>-0.016506</td>\n      <td>-0.225822</td>\n      <td>0.437507</td>\n      <td>0.045921</td>\n      <td>0.111977</td>\n      <td>-0.120557</td>\n      <td>-0.011100</td>\n      <td>-0.381664</td>\n      <td>-0.149901</td>\n      <td>0.482652</td>\n      <td>-0.580711</td>\n      <td>-1.199153</td>\n      <td>-0.604071</td>\n      <td>0.081929</td>\n      <td>0.011340</td>\n      <td>-0.446074</td>\n      <td>0.139542</td>\n      <td>-0.260829</td>\n      <td>0.131684</td>\n      <td>0.071712</td>\n      <td>-0.060887</td>\n      <td>-0.281729</td>\n      <td>-1.003867</td>\n      <td>1.059399</td>\n      <td>0.416818</td>\n      <td>0.365278</td>\n      <td>0.537678</td>\n      <td>2518.11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>62882</td>\n      <td>-0.330014</td>\n      <td>-0.161835</td>\n      <td>1.288293</td>\n      <td>-0.591877</td>\n      <td>0.289209</td>\n      <td>-0.283643</td>\n      <td>0.623377</td>\n      <td>-0.084072</td>\n      <td>0.080601</td>\n      <td>0.396721</td>\n      <td>-0.932837</td>\n      <td>0.612180</td>\n      <td>-0.103629</td>\n      <td>1.097272</td>\n      <td>0.943783</td>\n      <td>0.604135</td>\n      <td>0.405033</td>\n      <td>-0.035865</td>\n      <td>0.048024</td>\n      <td>-0.160212</td>\n      <td>-0.286617</td>\n      <td>-1.276665</td>\n      <td>0.294000</td>\n      <td>0.687970</td>\n      <td>0.151482</td>\n      <td>-0.168056</td>\n      <td>-0.218770</td>\n      <td>-0.032723</td>\n      <td>21222.90</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>414847</td>\n      <td>-1.998296</td>\n      <td>2.563713</td>\n      <td>-1.922748</td>\n      <td>2.220722</td>\n      <td>-2.315012</td>\n      <td>0.163451</td>\n      <td>-2.788664</td>\n      <td>-1.133662</td>\n      <td>-2.497692</td>\n      <td>-2.322209</td>\n      <td>1.274140</td>\n      <td>-1.696605</td>\n      <td>-0.190928</td>\n      <td>-0.993515</td>\n      <td>-1.318173</td>\n      <td>-1.629526</td>\n      <td>-1.788834</td>\n      <td>-1.798008</td>\n      <td>0.873544</td>\n      <td>2.070130</td>\n      <td>-2.119779</td>\n      <td>2.568182</td>\n      <td>1.578465</td>\n      <td>0.802100</td>\n      <td>-1.068481</td>\n      <td>0.383964</td>\n      <td>-2.780941</td>\n      <td>-1.372040</td>\n      <td>11009.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>37898</td>\n      <td>-0.239565</td>\n      <td>-0.914608</td>\n      <td>2.236272</td>\n      <td>-0.153059</td>\n      <td>0.143797</td>\n      <td>0.637981</td>\n      <td>0.430692</td>\n      <td>-0.160555</td>\n      <td>-0.179968</td>\n      <td>0.905365</td>\n      <td>-0.953598</td>\n      <td>0.544469</td>\n      <td>1.005150</td>\n      <td>0.474800</td>\n      <td>1.673229</td>\n      <td>-0.499807</td>\n      <td>0.724782</td>\n      <td>1.503645</td>\n      <td>0.070229</td>\n      <td>0.399908</td>\n      <td>-0.099731</td>\n      <td>0.203341</td>\n      <td>0.361850</td>\n      <td>0.172179</td>\n      <td>0.371764</td>\n      <td>0.139097</td>\n      <td>-0.336160</td>\n      <td>-0.325017</td>\n      <td>3200.31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>443820</td>\n      <td>-1.153168</td>\n      <td>0.398433</td>\n      <td>-1.029774</td>\n      <td>0.888771</td>\n      <td>-0.887774</td>\n      <td>-0.131860</td>\n      <td>-0.989098</td>\n      <td>-0.051896</td>\n      <td>-0.977661</td>\n      <td>-1.191869</td>\n      <td>1.265965</td>\n      <td>-1.217546</td>\n      <td>-1.535552</td>\n      <td>-1.230553</td>\n      <td>0.195721</td>\n      <td>-1.363702</td>\n      <td>-1.391726</td>\n      <td>-1.289715</td>\n      <td>1.461207</td>\n      <td>-0.227530</td>\n      <td>-0.144973</td>\n      <td>0.412875</td>\n      <td>0.737041</td>\n      <td>-0.849788</td>\n      <td>0.205872</td>\n      <td>-0.166221</td>\n      <td>1.127098</td>\n      <td>-0.046447</td>\n      <td>10421.22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>382863</td>\n      <td>-2.397646</td>\n      <td>2.894551</td>\n      <td>-2.311456</td>\n      <td>1.245323</td>\n      <td>-2.908986</td>\n      <td>-2.658784</td>\n      <td>-2.240014</td>\n      <td>4.794864</td>\n      <td>-1.120960</td>\n      <td>-1.246921</td>\n      <td>1.090825</td>\n      <td>-1.011093</td>\n      <td>0.921382</td>\n      <td>-0.870346</td>\n      <td>-0.111151</td>\n      <td>-1.193785</td>\n      <td>-1.401722</td>\n      <td>-1.438677</td>\n      <td>0.523435</td>\n      <td>1.433718</td>\n      <td>0.562760</td>\n      <td>-1.587140</td>\n      <td>-1.146213</td>\n      <td>0.341678</td>\n      <td>2.997600</td>\n      <td>-0.511209</td>\n      <td>1.774353</td>\n      <td>0.902366</td>\n      <td>17492.76</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>28344</td>\n      <td>1.048048</td>\n      <td>-0.559085</td>\n      <td>0.615327</td>\n      <td>-0.476877</td>\n      <td>0.077899</td>\n      <td>0.555793</td>\n      <td>0.272981</td>\n      <td>-0.122176</td>\n      <td>-0.119740</td>\n      <td>1.118909</td>\n      <td>-0.169065</td>\n      <td>0.710064</td>\n      <td>-0.395981</td>\n      <td>1.034372</td>\n      <td>0.469603</td>\n      <td>-0.124207</td>\n      <td>0.358027</td>\n      <td>1.669389</td>\n      <td>-1.292892</td>\n      <td>-0.783717</td>\n      <td>-0.200045</td>\n      <td>-0.241792</td>\n      <td>-0.072130</td>\n      <td>-0.604005</td>\n      <td>0.709923</td>\n      <td>-0.573698</td>\n      <td>-0.179990</td>\n      <td>-0.076768</td>\n      <td>5876.72</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>35303</td>\n      <td>-0.247323</td>\n      <td>-0.211047</td>\n      <td>1.250432</td>\n      <td>-0.404014</td>\n      <td>0.372625</td>\n      <td>0.886330</td>\n      <td>0.370228</td>\n      <td>0.071564</td>\n      <td>0.174766</td>\n      <td>0.484641</td>\n      <td>-0.602041</td>\n      <td>0.321745</td>\n      <td>-1.612295</td>\n      <td>1.408992</td>\n      <td>1.475467</td>\n      <td>0.774833</td>\n      <td>0.330033</td>\n      <td>0.927388</td>\n      <td>0.488063</td>\n      <td>0.002038</td>\n      <td>-0.146362</td>\n      <td>-0.598806</td>\n      <td>-0.161802</td>\n      <td>-1.998599</td>\n      <td>0.262482</td>\n      <td>-0.927780</td>\n      <td>0.008611</td>\n      <td>-0.090029</td>\n      <td>20595.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>174089</td>\n      <td>1.775570</td>\n      <td>-0.358267</td>\n      <td>-0.099641</td>\n      <td>-0.117799</td>\n      <td>0.441439</td>\n      <td>-0.471585</td>\n      <td>0.738081</td>\n      <td>-0.259170</td>\n      <td>0.495893</td>\n      <td>0.782529</td>\n      <td>-1.288591</td>\n      <td>0.404075</td>\n      <td>-1.101746</td>\n      <td>1.419499</td>\n      <td>0.292122</td>\n      <td>0.265918</td>\n      <td>0.326166</td>\n      <td>0.135647</td>\n      <td>-0.553871</td>\n      <td>-0.496381</td>\n      <td>-0.055479</td>\n      <td>0.312707</td>\n      <td>-0.038118</td>\n      <td>0.073018</td>\n      <td>0.633971</td>\n      <td>-1.197648</td>\n      <td>-0.285051</td>\n      <td>-0.251860</td>\n      <td>7484.70</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>385153</td>\n      <td>-0.347976</td>\n      <td>0.351449</td>\n      <td>-0.665557</td>\n      <td>0.920434</td>\n      <td>-0.494854</td>\n      <td>-0.699403</td>\n      <td>-0.757664</td>\n      <td>0.199784</td>\n      <td>-1.116544</td>\n      <td>-1.106623</td>\n      <td>1.108233</td>\n      <td>-1.220094</td>\n      <td>0.982276</td>\n      <td>-1.392358</td>\n      <td>-0.103015</td>\n      <td>-1.309980</td>\n      <td>-1.539134</td>\n      <td>-1.380626</td>\n      <td>0.311462</td>\n      <td>0.816260</td>\n      <td>0.357058</td>\n      <td>0.034090</td>\n      <td>-0.159199</td>\n      <td>-0.342399</td>\n      <td>1.486273</td>\n      <td>0.608609</td>\n      <td>1.727343</td>\n      <td>1.143565</td>\n      <td>8184.55</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>464790</td>\n      <td>-1.433882</td>\n      <td>1.413067</td>\n      <td>-1.479843</td>\n      <td>1.761009</td>\n      <td>-1.723727</td>\n      <td>-1.397144</td>\n      <td>-1.784354</td>\n      <td>1.057339</td>\n      <td>-1.727034</td>\n      <td>-1.860526</td>\n      <td>1.553582</td>\n      <td>-1.858695</td>\n      <td>0.182378</td>\n      <td>-1.316269</td>\n      <td>-1.488809</td>\n      <td>-1.945002</td>\n      <td>-2.150259</td>\n      <td>-2.206676</td>\n      <td>1.250607</td>\n      <td>-0.148611</td>\n      <td>0.462182</td>\n      <td>-0.140126</td>\n      <td>0.479393</td>\n      <td>1.226030</td>\n      <td>-0.665870</td>\n      <td>-0.842231</td>\n      <td>-1.004705</td>\n      <td>-0.218917</td>\n      <td>15624.23</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>82367</td>\n      <td>-0.294221</td>\n      <td>-0.972846</td>\n      <td>1.972505</td>\n      <td>-1.439339</td>\n      <td>-0.306559</td>\n      <td>1.360050</td>\n      <td>0.800627</td>\n      <td>-0.043037</td>\n      <td>0.022677</td>\n      <td>0.339720</td>\n      <td>-0.099597</td>\n      <td>0.512430</td>\n      <td>-0.743583</td>\n      <td>0.413766</td>\n      <td>-1.371178</td>\n      <td>0.911294</td>\n      <td>0.743846</td>\n      <td>-0.468919</td>\n      <td>-0.312641</td>\n      <td>0.735708</td>\n      <td>0.056936</td>\n      <td>0.418919</td>\n      <td>0.799179</td>\n      <td>-0.553536</td>\n      <td>0.535914</td>\n      <td>-0.708620</td>\n      <td>-0.300779</td>\n      <td>0.091472</td>\n      <td>15521.20</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>229226</td>\n      <td>0.016539</td>\n      <td>-0.159624</td>\n      <td>0.607801</td>\n      <td>-1.126025</td>\n      <td>0.869994</td>\n      <td>0.277329</td>\n      <td>1.238909</td>\n      <td>-0.341875</td>\n      <td>1.031074</td>\n      <td>0.303010</td>\n      <td>-1.116769</td>\n      <td>0.231496</td>\n      <td>-0.578925</td>\n      <td>-0.110102</td>\n      <td>-0.373550</td>\n      <td>0.392097</td>\n      <td>0.684862</td>\n      <td>0.537014</td>\n      <td>-0.086618</td>\n      <td>-0.155895</td>\n      <td>-0.244091</td>\n      <td>-0.393633</td>\n      <td>-0.330166</td>\n      <td>0.721116</td>\n      <td>-0.319584</td>\n      <td>-0.589745</td>\n      <td>-0.973118</td>\n      <td>-1.317766</td>\n      <td>16194.41</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"for_prediction = data_for_prediction.drop(['id','Class'], axis=1)\nfor_prediction = scaler.transform(for_prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:56:46.968259Z","iopub.execute_input":"2025-02-22T01:56:46.968536Z","iopub.status.idle":"2025-02-22T01:56:46.975048Z","shell.execute_reply.started":"2025-02-22T01:56:46.968516Z","shell.execute_reply":"2025-02-22T01:56:46.974156Z"},"id":"PHo18mRqo0nc"},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"","metadata":{"id":"x_wMFfSso0nc"}},{"cell_type":"markdown","source":"","metadata":{"id":"zI4q1mRao0nc"}},{"cell_type":"markdown","source":"# 7. Save model","metadata":{"id":"uVGbwNnEo0nq"}},{"cell_type":"code","source":" model.save('credit_card_fraud_detect_NN_model.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:57:11.809109Z","iopub.execute_input":"2025-02-22T01:57:11.809406Z","iopub.status.idle":"2025-02-22T01:57:11.857481Z","shell.execute_reply.started":"2025-02-22T01:57:11.809382Z","shell.execute_reply":"2025-02-22T01:57:11.856859Z"},"id":"aDMXjGalo0nq"},"outputs":[],"execution_count":27},{"cell_type":"code","source":"for i, (record, label) in enumerate(zip(for_prediction, data_for_prediction['Class'])):\n    # Reshape the record to match the model input shape (1, num_features)\n    record_reshaped = record.reshape(1, -1)\n    prediction = model.predict(record_reshaped)\n    threshold = 0.5\n\n    predicted_class = (prediction > threshold).astype(int)\n    print(f\"predicted_class raw data : {predicted_class}\")\n    print(f\"Record {i + 1}:\")\n    print(f\"  Actual Label: {label}\")\n    print(f\"  Scaled Input: {record}\")\n    print(f\"  Raw Prediction (Probability): {prediction[0][0]}\")\n    print(f\"  Predicted Class: {predicted_class[0][0]}\")\n    print(\"-\" * 50)\n    break  #remove this break, if you want to see that how prediction gets match on all records of unseen data.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T01:57:35.379627Z","iopub.execute_input":"2025-02-22T01:57:35.379927Z","iopub.status.idle":"2025-02-22T01:57:35.440949Z","shell.execute_reply.started":"2025-02-22T01:57:35.379903Z","shell.execute_reply":"2025-02-22T01:57:35.440315Z"},"id":"0Q2mv-AGo0nr","outputId":"acc92e86-0f85-4f2c-c001-f6c601e326a7"},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\npredicted_class raw data : [[1]]\nRecord 1:\n  Actual Label: 1\n  Scaled Input: [ 0.4201251  -0.07010059 -0.56946044  0.19167551 -0.00950122  0.42721494\n -0.35446989  0.0962281   0.07782132 -0.67408304  0.58670638 -0.75131322\n -1.03386924 -0.88721337  0.54294162 -1.0746347  -0.66493438 -1.14827143\n -0.1682811   0.2357356   0.13526966  0.07009707  0.0474236  -0.85254013\n  0.1031686  -0.37536891  0.81896055  0.66655887 -0.49244815]\n  Raw Prediction (Probability): 0.9967625141143799\n  Predicted Class: 1\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":29}]}